{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb9abc5f",
   "metadata": {},
   "source": [
    "## Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1639ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import argparse\n",
    "import os\n",
    "from typing import List, Dict, Optional, Union\n",
    "from google import  genai\n",
    "from google.genai import types\n",
    "from io import BytesIO\n",
    "from datasets import load_dataset\n",
    "import google.generativeai as genai\n",
    "from PIL import Image\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f87db76",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bde0df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLoader:\n",
    "    def __init__(self, dataset_name: str = \"AI4Math/MathVista\"):\n",
    "        self.dataset_name = dataset_name\n",
    "\n",
    "    def load_mathvista_dataset(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Load MathVista dataset from Hugging Face or return sample data if loading fails.\"\"\"\n",
    "        dataset = load_dataset(dataset_name)[\"testmini\"]\n",
    "        return [dict(item) for item in dataset]\n",
    "    \n",
    "    def select_examples(self, dataset: List[Dict[str, Any]], num_examples: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Randomly select examples from the dataset.\"\"\"\n",
    "        return dataset if len(dataset) <= num_examples else random.sample(dataset, num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e8c312",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### Example Usage of DatasetLoader ####################\n",
    "\n",
    "# dataset_loader = DatasetLoader()\n",
    "# mathvista_data = dataset_loader.load_mathvista_dataset()\n",
    "# few_shot_examples = dataset_loader.select_examples(mathvista_data, num_examples=3)\n",
    "\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3673e45",
   "metadata": {},
   "source": [
    "## Load the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dc7351",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM:\n",
    "    def __init__(self, model_name: str = \"gemini-2.0-flash-exp\", temperature: float = 0.0, system_prompt: str = \"\"):\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.system_prompt = system_prompt\n",
    "        \n",
    "        # Configure the API using the key from environment variable\n",
    "        api_key = os.getenv(\"GOOGLE_GENAI_API_KEY\")\n",
    "        if api_key:\n",
    "            genai.configure(api_key=api_key)\n",
    "        else:\n",
    "            raise ValueError(\"GOOGLE_GENAI_API_KEY environment variable not set.\")\n",
    "        \n",
    "        # Create the model instance\n",
    "        generation_config = {\n",
    "            \"temperature\": self.temperature,\n",
    "        }\n",
    "        \n",
    "        self.model = genai.GenerativeModel(\n",
    "            model_name=self.model_name,\n",
    "            generation_config=generation_config,\n",
    "            system_instruction=self.system_prompt if self.system_prompt else None\n",
    "        )\n",
    "\n",
    "    def _format_few_shot_examples(self, few_shot_examples: List[Dict[str, Union[str, bytes]]], include_images: bool = True) -> str:\n",
    "        \"\"\"Format few-shot examples into a string for the prompt.\"\"\"\n",
    "        if not few_shot_examples:\n",
    "            return \"\"\n",
    "        \n",
    "        formatted_examples = [\"Here are some examples:\"]\n",
    "        \n",
    "        for i, example in enumerate(few_shot_examples, 1):\n",
    "            formatted_examples.append(f\"\\nExample {i}:\")\n",
    "            if include_images and \"image\" in example:\n",
    "                formatted_examples.append(\"Input: [Image provided]\")\n",
    "            if \"input\" in example:\n",
    "                formatted_examples.append(f\"Input: {example['input']}\")\n",
    "            if \"output\" in example:\n",
    "                formatted_examples.append(f\"Output: {example['output']}\")\n",
    "        \n",
    "        formatted_examples.append(\"\\nNow, please respond to the following:\")\n",
    "        return \"\\n\".join(formatted_examples)\n",
    "\n",
    "    def generate_response_with_image(self, prompt: str, image_bytes: bytes, \n",
    "                                   few_shot_examples: Optional[List[Dict[str, Union[str, bytes]]]] = None) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response from the LLM based on the provided prompt and image bytes.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The text prompt\n",
    "            image_bytes: Image data as bytes\n",
    "            few_shot_examples: Optional list of examples. Each example should be a dict with:\n",
    "                - 'input': input text (optional)\n",
    "                - 'output': expected output text\n",
    "                - 'image': image bytes (optional)\n",
    "        \n",
    "        Returns:\n",
    "            Generated response as string\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert bytes to PIL Image\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            \n",
    "            # Prepare content list\n",
    "            content = []\n",
    "            \n",
    "            # Add few-shot examples if provided\n",
    "            if few_shot_examples:\n",
    "                # example_prompt = self._format_few_shot_examples(few_shot_examples, include_images=True)\n",
    "                # content.append(example_prompt)\n",
    "                \n",
    "                # Add example images if they exist\n",
    "                for example in few_shot_examples:\n",
    "                    if \"image\" in example:\n",
    "                        example_image = Image.open(io.BytesIO(example[\"image\"]))\n",
    "                        content.append(example_image)\n",
    "            \n",
    "            # Add the main prompt and image\n",
    "            content.extend([prompt, image])\n",
    "            \n",
    "            # Generate response\n",
    "            response = self.model.generate_content(content)\n",
    "            \n",
    "            return response.text\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "    def generate_response_without_image(self, prompt: str, few_shot_examples: Optional[List[Dict[str, str]]] = None) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response from the LLM based on the provided prompt (text only).\n",
    "        \n",
    "        Args:\n",
    "            prompt: The text prompt\n",
    "            few_shot_examples: Optional list of examples. Each example should be a dict with:\n",
    "                - 'input': input text\n",
    "                - 'output': expected output text\n",
    "        \n",
    "        Returns:\n",
    "            Generated response as string\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # gieve few-shot examples as content if provided without formatting few-shot examples\n",
    "            content = []\n",
    "            if few_shot_examples:\n",
    "                for example in few_shot_examples:\n",
    "                    content.append(f\"Input: {example['input']}\\nOutput: {example['output']}\")\n",
    "            content.append(prompt)\n",
    "\n",
    "            # Generate response\n",
    "            response = self.model.generate_content(content)\n",
    "\n",
    "            return response.text\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "    def generate_response(self, prompt: str, few_shot_examples: Optional[List[Dict[str, str]]] = None, image_bytes: Optional[bytes] = None) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response from the LLM based on the provided prompt.\n",
    "        If image_bytes is provided, it will include the image in the prompt.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The text prompt\n",
    "            few_shot_examples: Optional list of examples. Each example should be a dict with:\n",
    "                - 'input': input text\n",
    "                - 'output': expected output text\n",
    "                - 'image': image bytes (optional)\n",
    "            image_bytes: Optional image data as bytes   \n",
    "        Returns:\n",
    "            Generated response as string\n",
    "        \"\"\"\n",
    "        if image_bytes:\n",
    "            return self.generate_response_with_image(prompt, image_bytes, few_shot_examples)\n",
    "        else:\n",
    "            return self.generate_response_without_image(prompt, few_shot_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a72f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Example usage ##################\n",
    "\n",
    "# # Initialize the LLM\n",
    "# llm = LLM(system_prompt=\"You are a helpful assistant\")\n",
    "\n",
    "# # Text-only with few-shot examples\n",
    "# text_examples = [\n",
    "#     {\"input\": \"What is 2+2?\", \"output\": \"2+2 equals 4\"},\n",
    "#     {\"input\": \"What is 3+3?\", \"output\": \"3+3 equals 6\"}\n",
    "# ]\n",
    "# response = llm.generate_response(\"What is 5+5?\", few_shot_examples=text_examples)\n",
    "\n",
    "# # Image with few-shot examples\n",
    "# image_examples = [\n",
    "#     {\"input\": \"Describe this image\", \"output\": \"This is a cat\", \"image\": example_image_bytes},\n",
    "#     {\"output\": \"This is a dog\", \"image\": another_example_image_bytes}\n",
    "# ]\n",
    "# response = llm.generate_response_with_image(\"What do you see?\", image_bytes, few_shot_examples=image_examples)\n",
    "\n",
    "# # Without examples\n",
    "# response = llm.generate_response(\"Hello, how are you?\")\n",
    "# response = llm.generate_response_with_image(\"What's in this image?\", image_bytes)\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ae34ac",
   "metadata": {},
   "source": [
    "## Idea Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3c445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdeaGenerator:\n",
    "    def __init__(self, system_prompt: str = \"\"):\n",
    "        self.llm = LLM(system_prompt=system_prompt)\n",
    "\n",
    "    def generate_idea(self, difficulty_level: int, given_example: Dict[str, Any]) -> str:\n",
    "        \"\"\"\n",
    "        Extend the current example to generate a similar math problem idea with the specified difficulty level.\n",
    "        positive integer difficulty_level: Increases difficulty level for the new problem.\n",
    "        negative integer difficulty_level: Decreases difficulty level for the new problem.\n",
    "        zero difficulty_level: Keeps the same difficulty level for the new problem.\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Given the example.\n",
    "        Generate a new math problem idea for difficulty level {difficulty_level}.\n",
    "        positive integer difficulty_level: Increases difficulty level for the new problem.\n",
    "        negative integer difficulty_level: Decreases difficulty level for the new problem.\n",
    "        zero difficulty_level: Keeps the same difficulty level for the new problem.\n",
    "        \"\"\"\n",
    "        llm_response = self.llm.generate_response(prompt, few_shot_examples=[given_example])\n",
    "        return llm_response\n",
    "\n",
    "    def generate_question_template(self, idea: str, difficulty_level: int, given_example: Dict[str, Any]) -> str:\n",
    "        \"\"\"\n",
    "        Generate a question template based on the provided idea and difficulty level.\n",
    "        positive integer difficulty_level: Increases difficulty level for the new problem.\n",
    "        negative integer difficulty_level: Decreases difficulty level for the new problem.\n",
    "        zero difficulty_level: Keeps the same difficulty level for the new problem.\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Given the idea and the example.\n",
    "        Create a question template for the following idea at difficulty level {difficulty_level}:\n",
    "        Idea: {idea}\n",
    "        positive integer difficulty_level: Increases difficulty level for the new problem.\n",
    "        negative integer difficulty_level: Decreases difficulty level for the new problem.\n",
    "        zero difficulty_level: Keeps the same difficulty level for the new problem.\n",
    "        \"\"\"\n",
    "        llm_response = self.llm.generate_response(prompt, few_shot_examples=[given_example])\n",
    "        return llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eda167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e132f50",
   "metadata": {},
   "source": [
    "## Programatically generating Image Using VLM\n",
    "### coding model generates code to generate the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84852b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageGenerator:\n",
    "    def __init__(self, system_prompt: str = \"\"):\n",
    "        self.llm = LLM(system_prompt=system_prompt)\n",
    "    \n",
    "    def generate_code(self, prompt: str) -> str:\n",
    "        llm_response = self.llm.generate_response(prompt)\n",
    "        return llm_response\n",
    "    def save_code_to_file(self, code: str, filename: str = \"generated_code.py\"):\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(code)\n",
    "    def execute_code(self, filename: str = \"generated_code.py\") -> bytes:\n",
    "        import importlib.util\n",
    "        spec = importlib.util.spec_from_file_location(\"generated_code\", filename)\n",
    "        generated_code = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(generated_code)\n",
    "        if hasattr(generated_code, \"generate_image\"):\n",
    "            image_bytes = generated_code.generate_image()\n",
    "            return image_bytes\n",
    "        else:\n",
    "            raise AttributeError(\"The generated code does not have a 'generate_image' function.\")\n",
    "    def generate_image_and_save(self, prompt: str) -> bytes:\n",
    "        code = self.generate_code(prompt)\n",
    "        self.save_code_to_file(code)\n",
    "        image_bytes = self.execute_code()\n",
    "        #save image bytes to file for verification\n",
    "        with open(\"generated_image.png\", \"wb\") as img_file:\n",
    "            img_file.write(image_bytes)\n",
    "        return image_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07801bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Example usage ##########################\n",
    "# image_gen = ImageGenerator(system_prompt=\"You are a helpful assistant that generates Python code to\n",
    "# create images using matplotlib.\")\n",
    "# code_prompt = \"Generate Python code to create a plot of y=sin(x) from x=0 to 2π using matplotlib.\"\n",
    "# image_bytes = image_gen.generate_image_and_save(code_prompt)\n",
    "####################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e941a3",
   "metadata": {},
   "source": [
    "## Verifying the generate Image and Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59b7114",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Verifier:\n",
    "    def __init__(self, system_prompt: str = \"\"):\n",
    "        self.llm = LLM(system_prompt=system_prompt)\n",
    "    \n",
    "    def verify_question_and_image(self, question: str, image_bytes: bytes, expected_answer: str) -> str:\n",
    "        prompt = f\"\"\"\n",
    "        Verify if the question and the image correspond to the expected answer.\n",
    "        Question: {question}\n",
    "        Expected Answer: {expected_answer}\n",
    "        \"\"\"\n",
    "        llm_response = self.llm.generate_response_with_image(prompt, image_bytes)\n",
    "        return llm_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cb4279",
   "metadata": {},
   "source": [
    "## Save it to the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca0e7f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d681a797",
   "metadata": {},
   "source": [
    "## Pipeline execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d388406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
